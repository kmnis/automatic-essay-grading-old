{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:50:51.592342Z",
     "start_time": "2021-06-13T16:50:51.576933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:50:08.285536Z",
     "start_time": "2021-06-13T16:50:07.951852Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12976, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/training_set_rel3.tsv\", delimiter=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:50:18.323365Z",
     "start_time": "2021-06-13T16:50:10.171559Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay     score\n",
       "0  Dear local newspaper, I think effects computer...  0.666667\n",
       "1  Dear @CAPS1 @CAPS2, I believe that using compu...  0.750000\n",
       "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  0.583333\n",
       "3  Dear Local Newspaper, @CAPS1 I have found that...  0.833333\n",
       "4  Dear @LOCATION1, I know having computers has a...  0.666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'score'] = 0\n",
    "\n",
    "# For essay_set = 1\n",
    "ind = df.index[df['essay_set']==1]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/12\n",
    "\n",
    "# For essay_set = 2\n",
    "ind = df.index[df['essay_set']==2]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = ((df['domain1_score'][i]/6) + (df['domain2_score'][i]/4))/2\n",
    "\n",
    "# For essay_set = 3\n",
    "ind = df.index[df['essay_set']==3]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/3\n",
    "\n",
    "# For essay_set = 4\n",
    "ind = df.index[df['essay_set']==4]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/3\n",
    "\n",
    "# For essay_set = 5\n",
    "ind = df.index[df['essay_set']==5]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/4\n",
    "\n",
    "# For essay_set = 6\n",
    "ind = df.index[df['essay_set']==6]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/4\n",
    "\n",
    "# For essay_set = 7\n",
    "ind = df.index[df['essay_set']==7]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/30\n",
    "\n",
    "# For essay_set = 8\n",
    "ind = df.index[df['essay_set']==8]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/60\n",
    "\n",
    "df = df[[\"essay\", \"score\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T17:01:17.283414Z",
     "start_time": "2021-06-13T17:01:17.177321Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# A list of contractions from https://stackoverflow.com/q/19790188/9865225\n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    \"\"\"Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\"\"\"\n",
    "\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace contractions with their longer forms\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?://.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count the number of occurrences of each word in a set of text\"\"\"\n",
    "    count_dict = defaultdict(int)\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            count_dict[word] += 1\n",
    "    return dict(count_dict)\n",
    "\n",
    "\n",
    "def convert_to_ints(text, word_count, unk_count, vocab_to_int, eos=False):\n",
    "    \"\"\"Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts\"\"\"\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count\n",
    "\n",
    "\n",
    "def load_embeddings(glove_dir):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_dir, encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=400000):\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def convert_vocab_to_int(word_counts, embeddings_index, threshold=20):\n",
    "    # Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "    # dictionary to convert words to integers\n",
    "    vocab_to_int = {}\n",
    "\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= threshold or word in embeddings_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "\n",
    "    # Special tokens that will be added to our vocab\n",
    "    codes = [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<GO>\"]\n",
    "\n",
    "    # Add codes to vocab\n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "    # Dictionary to convert integers to words\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts), 4) * 100\n",
    "\n",
    "    print(\"Total number of unique words:\", len(word_counts))\n",
    "    print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "    \n",
    "    return vocab_to_int\n",
    "\n",
    "\n",
    "def unk_counter(sentence, vocab_to_int):\n",
    "    \"\"\"Counts the number of time UNK appears in a sentence.\"\"\"\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count\n",
    "\n",
    "\n",
    "def clean_reviews(lengths_texts, int_texts, vocab_to_int):\n",
    "    # Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "    # Limit the length of summaries and texts based on the min and max ranges.\n",
    "    # Remove reviews that include too many UNKs\n",
    "\n",
    "    # sorted_summaries = []\n",
    "    sorted_texts = []\n",
    "    max_text_length = 150\n",
    "    # max_summary_length = 13\n",
    "    min_length = 2\n",
    "    unk_text_limit = 10\n",
    "    # unk_summary_limit = 0\n",
    "\n",
    "    for _ in tqdm(range(min(lengths_texts.counts), max_text_length)):\n",
    "        for count in range(len(int_texts)):\n",
    "            if (unk_counter(int_texts[count], vocab_to_int) <= unk_text_limit and len(\n",
    "                    int_texts[count]) >= min_length and len(int_texts[count]) <= max_text_length):\n",
    "                sorted_texts.append(int_texts[count])\n",
    "\n",
    "    # Compare lengths to ensure they match\n",
    "    print(len(sorted_texts))\n",
    "    return sorted_texts\n",
    "\n",
    "\n",
    "def create_lengths(text):\n",
    "    \"\"\"Create a data frame of the sentence lengths from a text\"\"\"\n",
    "    lengths = []\n",
    "    for sentence in tqdm(text):\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:51:04.393645Z",
     "start_time": "2021-06-13T16:50:59.204605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fa02cce8394521acae3948a01fb4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12976.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Essays are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_texts = []\n",
    "for text in tqdm(df.essay):\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Essays are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:51:43.195284Z",
     "start_time": "2021-06-13T16:51:42.648297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 39634\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "\n",
    "word_counts = count_words(clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:55:10.196827Z",
     "start_time": "2021-06-13T16:54:59.634552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84511c27c17a4414b9dac4df2f603591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings(\"../../Automatic Essay Grading/glove/glove.6B/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:55:41.377985Z",
     "start_time": "2021-06-13T16:55:41.239676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9ec9f448e14a71ba6ace19b6349db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39634.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words missing from glove: 266\n",
      "Percent of words that are missing from vocabulary: 0.67%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in tqdm(word_counts.items()):\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from glove:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a threshold of 20, so that words not in CN can be added to our word_embedding_matrix, but they need to be common enough in the reviews so that the model can understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:57:40.241842Z",
     "start_time": "2021-06-13T16:57:40.176509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 39634\n",
      "Number of words we will use: 21358\n",
      "Percent of words we will use: 53.89000000000001%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = convert_vocab_to_int(word_counts, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:57:43.032505Z",
     "start_time": "2021-06-13T16:57:42.819985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565c223055246d8a1a87155ac6305d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21358.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21358\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 50\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in tqdm(vocab_to_int.items()):\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:00.438066Z",
     "start_time": "2021-06-13T16:57:59.719759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1407079\n",
      "Total number of UNKs in headlines: 29701\n",
      "Percent of words that are UNK: 2.11%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, vocab_to_int, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:05.352208Z",
     "start_time": "2021-06-13T16:58:05.181822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9d0ec304a474ca977865e46c31fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12976.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  12976.000000\n",
      "mean     109.437038\n",
      "std       85.060288\n",
      "min        2.000000\n",
      "25%       48.000000\n",
      "50%       83.000000\n",
      "75%      149.000000\n",
      "max      525.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_texts = create_lengths(int_texts)\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:10.320623Z",
     "start_time": "2021-06-13T16:58:10.312324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.0\n",
      "286.0\n",
      "393.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T17:02:12.054556Z",
     "start_time": "2021-06-13T17:01:51.512350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a78422f092c45a1b948e644f0094a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1430864\n"
     ]
    }
   ],
   "source": [
    "sorted_texts = clean_reviews(lengths_texts, int_texts, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
