{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:39:18.014879Z",
     "start_time": "2021-06-22T20:39:14.720752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.15.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:05:23.977835Z",
     "start_time": "2021-06-23T21:05:23.661128Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12976, 28)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/raw/training_set_rel3.tsv\", delimiter=\"\\t\", encoding=\"ISO-8859-1\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:12:33.204029Z",
     "start_time": "2021-06-23T21:12:33.190719Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/betelgeuse/Desktop/projects/automatic-essay-grading/notebooks'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.dirname(os.path.realpath('__file__'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:06:12.463642Z",
     "start_time": "2021-06-23T21:06:11.953382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.boxplot(df.score)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T21:05:35.691133Z",
     "start_time": "2021-06-23T21:05:26.241518Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               essay     score\n",
       "0  Dear local newspaper, I think effects computer...  0.666667\n",
       "1  Dear @CAPS1 @CAPS2, I believe that using compu...  0.750000\n",
       "2  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...  0.583333\n",
       "3  Dear Local Newspaper, @CAPS1 I have found that...  0.833333\n",
       "4  Dear @LOCATION1, I know having computers has a...  0.666667"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[:, 'score'] = 0\n",
    "\n",
    "# For essay_set = 1\n",
    "ind = df.index[df['essay_set']==1]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/12\n",
    "\n",
    "# For essay_set = 2\n",
    "ind = df.index[df['essay_set']==2]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = ((df['domain1_score'][i]/6) + (df['domain2_score'][i]/4))/2\n",
    "\n",
    "# For essay_set = 3\n",
    "ind = df.index[df['essay_set']==3]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/3\n",
    "\n",
    "# For essay_set = 4\n",
    "ind = df.index[df['essay_set']==4]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/3\n",
    "\n",
    "# For essay_set = 5\n",
    "ind = df.index[df['essay_set']==5]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/4\n",
    "\n",
    "# For essay_set = 6\n",
    "ind = df.index[df['essay_set']==6]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/4\n",
    "\n",
    "# For essay_set = 7\n",
    "ind = df.index[df['essay_set']==7]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/30\n",
    "\n",
    "# For essay_set = 8\n",
    "ind = df.index[df['essay_set']==8]\n",
    "for i in ind:\n",
    "    df.loc[i,'score'] = df['domain1_score'][i]/60\n",
    "\n",
    "df = df[[\"essay\", \"score\"]]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-22T20:41:23.227930Z",
     "start_time": "2021-06-22T20:41:23.182512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T20:27:49.732124Z",
     "start_time": "2021-06-23T20:27:43.139295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb5056e574b4daaa2b8faec95479a5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12976.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1407133 39605\n"
     ]
    }
   ],
   "source": [
    "docs_text = []\n",
    "\n",
    "for essay in tqdm(df['essay'], total=len(df)):\n",
    "    essay = clean_text(essay)\n",
    "    word_sequence = text_to_word_sequence(essay)\n",
    "    docs_text.extend(word_sequence)\n",
    "\n",
    "print(len(docs_text), len(set(docs_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T20:02:32.828446Z",
     "start_time": "2021-06-23T20:02:32.756184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4989, 20932, 16)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = {}\n",
    "for word, count in tokenizer.word_counts.items():\n",
    "    if count >= 16:\n",
    "        word_counts[word] = count\n",
    "\n",
    "len(word_counts), max(word_counts.values()), min(word_counts.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T20:14:40.668087Z",
     "start_time": "2021-06-23T20:14:40.654099Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3., 1.])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix([\"dear book hello\", \"how are you speaker\"]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T19:55:43.596694Z",
     "start_time": "2021-06-23T19:55:30.303340Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(docs_text)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Increment the index by 1 to take padding token at index 0\n",
    "word_index = {word: index + 1 for word, index in word_index.items()}\n",
    "\n",
    "# Add padding and unkown token\n",
    "# word_index['<PAD>'] = 0\n",
    "# word_index['<UNK>'] = len(word_index)\n",
    "\n",
    "index_word = {index: word for word, index in word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T20:27:25.094196Z",
     "start_time": "2021-06-23T20:27:25.078399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39605"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-23T19:51:12.270039Z",
     "start_time": "2021-06-23T19:51:12.154496Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# A list of contractions from https://stackoverflow.com/q/19790188/9865225\n",
    "contractions = {\n",
    "    \"ain't\": \"am not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"hadn't've\": \"had not have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"he'd've\": \"he would have\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"how'd\": \"how did\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how's\": \"how is\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"sha'n't\": \"shall not\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"that'd\": \"that would\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"there'd\": \"there had\",\n",
    "    \"there's\": \"there is\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"they'll\": \"they will\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'll\": \"what will\",\n",
    "    \"what're\": \"what are\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who'll\": \"who will\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\"\n",
    "}\n",
    "\n",
    "\n",
    "def clean_text(text, remove_stopwords=True):\n",
    "    \"\"\"Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\"\"\"\n",
    "\n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "\n",
    "    # Replace contractions with their longer forms\n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "\n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?://.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text)\n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if w not in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count the number of occurrences of each word in a set of text\"\"\"\n",
    "    count_dict = defaultdict(int)\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            count_dict[word] += 1\n",
    "    return dict(count_dict)\n",
    "\n",
    "\n",
    "def convert_to_ints(text, word_count, unk_count, vocab_to_int, eos=False):\n",
    "    \"\"\"Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts\"\"\"\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count\n",
    "\n",
    "\n",
    "def load_embeddings(glove_dir):\n",
    "    embeddings_index = {}\n",
    "    with open(glove_dir, encoding='utf-8') as f:\n",
    "        for line in tqdm(f, total=400000):\n",
    "            values = line.split(' ')\n",
    "            word = values[0]\n",
    "            embedding = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = embedding\n",
    "\n",
    "    print('Word embeddings:', len(embeddings_index))\n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def convert_vocab_to_int(word_counts, embeddings_index, threshold=20):\n",
    "    # Limit the vocab that we will use to words that appear ≥ threshold or are in GloVe\n",
    "\n",
    "    # dictionary to convert words to integers\n",
    "    vocab_to_int = {}\n",
    "\n",
    "    value = 0\n",
    "    for word, count in word_counts.items():\n",
    "        if count >= threshold or word in embeddings_index:\n",
    "            vocab_to_int[word] = value\n",
    "            value += 1\n",
    "\n",
    "    # Special tokens that will be added to our vocab\n",
    "    codes = [\"<UNK>\", \"<PAD>\", \"<EOS>\", \"<GO>\"]\n",
    "\n",
    "    # Add codes to vocab\n",
    "    for code in codes:\n",
    "        vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "    # Dictionary to convert integers to words\n",
    "    int_to_vocab = {}\n",
    "    for word, value in vocab_to_int.items():\n",
    "        int_to_vocab[value] = word\n",
    "\n",
    "    usage_ratio = round(len(vocab_to_int) / len(word_counts), 4) * 100\n",
    "\n",
    "    print(\"Total number of unique words:\", len(word_counts))\n",
    "    print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "    print(\"Percent of words we will use: {}%\".format(usage_ratio))\n",
    "    \n",
    "    return vocab_to_int\n",
    "\n",
    "\n",
    "def unk_counter(sentence, vocab_to_int):\n",
    "    \"\"\"Counts the number of time UNK appears in a sentence.\"\"\"\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count\n",
    "\n",
    "\n",
    "def clean_reviews(lengths_texts, int_texts, vocab_to_int):\n",
    "    # Sort the summaries and texts by the length of the texts, shortest to longest\n",
    "    # Limit the length of summaries and texts based on the min and max ranges.\n",
    "    # Remove reviews that include too many UNKs\n",
    "\n",
    "    # sorted_summaries = []\n",
    "    sorted_texts = []\n",
    "    max_text_length = 150\n",
    "    # max_summary_length = 13\n",
    "    min_length = 2\n",
    "    unk_text_limit = 10\n",
    "    # unk_summary_limit = 0\n",
    "\n",
    "    for _ in tqdm(range(min(lengths_texts.counts), max_text_length)):\n",
    "        for count in range(len(int_texts)):\n",
    "            if (unk_counter(int_texts[count], vocab_to_int) <= unk_text_limit and len(\n",
    "                    int_texts[count]) >= min_length and len(int_texts[count]) <= max_text_length):\n",
    "                sorted_texts.append(int_texts[count])\n",
    "\n",
    "    # Compare lengths to ensure they match\n",
    "    print(len(sorted_texts))\n",
    "    return sorted_texts\n",
    "\n",
    "\n",
    "def create_lengths(text):\n",
    "    \"\"\"Create a data frame of the sentence lengths from a text\"\"\"\n",
    "    lengths = []\n",
    "    for sentence in tqdm(text):\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:51:04.393645Z",
     "start_time": "2021-06-13T16:50:59.204605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fa02cce8394521acae3948a01fb4bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12976.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Essays are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_texts = []\n",
    "for text in tqdm(df.essay):\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Essays are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:51:43.195284Z",
     "start_time": "2021-06-13T16:51:42.648297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 39634\n"
     ]
    }
   ],
   "source": [
    "# Find the number of times each word was used and the size of the vocabulary\n",
    "\n",
    "word_counts = count_words(clean_texts)\n",
    "            \n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:55:10.196827Z",
     "start_time": "2021-06-13T16:54:59.634552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84511c27c17a4414b9dac4df2f603591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word embeddings: 400000\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = load_embeddings(\"../../Automatic Essay Grading/glove/glove.6B/glove.6B.50d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:55:41.377985Z",
     "start_time": "2021-06-13T16:55:41.239676Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c9ec9f448e14a71ba6ace19b6349db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=39634.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words missing from glove: 266\n",
      "Percent of words that are missing from vocabulary: 0.67%\n"
     ]
    }
   ],
   "source": [
    "# Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in tqdm(word_counts.items()):\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from glove:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use a threshold of 20, so that words not in CN can be added to our word_embedding_matrix, but they need to be common enough in the reviews so that the model can understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:57:40.241842Z",
     "start_time": "2021-06-13T16:57:40.176509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 39634\n",
      "Number of words we will use: 21358\n",
      "Percent of words we will use: 53.89000000000001%\n"
     ]
    }
   ],
   "source": [
    "vocab_to_int = convert_vocab_to_int(word_counts, embeddings_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:57:43.032505Z",
     "start_time": "2021-06-13T16:57:42.819985Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a565c223055246d8a1a87155ac6305d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=21358.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "21358\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 50\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in tqdm(vocab_to_int.items()):\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:00.438066Z",
     "start_time": "2021-06-13T16:57:59.719759Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1407079\n",
      "Total number of UNKs in headlines: 29701\n",
      "Percent of words that are UNK: 2.11%\n"
     ]
    }
   ],
   "source": [
    "# Apply convert_to_ints to clean_summaries and clean_texts\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, vocab_to_int, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:05.352208Z",
     "start_time": "2021-06-13T16:58:05.181822Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a9d0ec304a474ca977865e46c31fa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=12976.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texts:\n",
      "             counts\n",
      "count  12976.000000\n",
      "mean     109.437038\n",
      "std       85.060288\n",
      "min        2.000000\n",
      "25%       48.000000\n",
      "50%       83.000000\n",
      "75%      149.000000\n",
      "max      525.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_texts = create_lengths(int_texts)\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T16:58:10.320623Z",
     "start_time": "2021-06-13T16:58:10.312324Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230.0\n",
      "286.0\n",
      "393.0\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 90))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-13T17:02:12.054556Z",
     "start_time": "2021-06-13T17:01:51.512350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a78422f092c45a1b948e644f0094a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=148.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1430864\n"
     ]
    }
   ],
   "source": [
    "sorted_texts = clean_reviews(lengths_texts, int_texts, vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
